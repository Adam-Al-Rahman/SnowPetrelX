{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sp_utils import update_config, pose_estimation, save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from rich import print\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import roi_align\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Device: cuda\n",
       "Device CUDNN enabled: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Device: cuda\n",
       "Device CUDNN enabled: \u001b[3;92mTrue\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Device-Agnostic\n",
    "DeviceLikeType = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(DeviceLikeType)\n",
    "print(f\"\"\"\n",
    "Device: {device}\n",
    "Device CUDNN enabled: {torch.backends.cudnn.enabled}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 320\n",
    "IMG_HEIGHT = 240\n",
    "NUM_KEYPOINTS = 7\n",
    "NUM_BATCH = 16\n",
    "\n",
    "CONFIG_PATH = \"config.json\"\n",
    "MODEL_PATH = \"models/pose_estimation\"\n",
    "DATASET_ROOT = \"datasets\"\n",
    "TRAIN_DATASET_FILE = DATASET_ROOT + \"/preprocessed_dataset.csv\"\n",
    "TEST_DATASET_FILE = DATASET_ROOT + \"/test_dataset.csv\"\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATASET_FILE)\n",
    "test_df = pd.read_csv(TEST_DATASET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>behavior</th>\n",
       "      <th>image_id</th>\n",
       "      <th>image_file</th>\n",
       "      <th>head_x</th>\n",
       "      <th>head_y</th>\n",
       "      <th>beak_base_x</th>\n",
       "      <th>beak_base_y</th>\n",
       "      <th>beak_tip_x</th>\n",
       "      <th>beak_tip_y</th>\n",
       "      <th>neck_x</th>\n",
       "      <th>neck_y</th>\n",
       "      <th>body1_x</th>\n",
       "      <th>body1_y</th>\n",
       "      <th>body2_x</th>\n",
       "      <th>body2_y</th>\n",
       "      <th>tail_base_x</th>\n",
       "      <th>tail_base_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>n_001</td>\n",
       "      <td>59-20151230231705-00.jpg</td>\n",
       "      <td>19.234443</td>\n",
       "      <td>92.112384</td>\n",
       "      <td>41.246921</td>\n",
       "      <td>134.089668</td>\n",
       "      <td>39.711167</td>\n",
       "      <td>149.447212</td>\n",
       "      <td>61.211727</td>\n",
       "      <td>86.993203</td>\n",
       "      <td>79.640779</td>\n",
       "      <td>136.137341</td>\n",
       "      <td>123.153818</td>\n",
       "      <td>131.530078</td>\n",
       "      <td>176.393301</td>\n",
       "      <td>7.133978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>n_001</td>\n",
       "      <td>59-20151230231706-00.jpg</td>\n",
       "      <td>12.579507</td>\n",
       "      <td>49.111263</td>\n",
       "      <td>43.806512</td>\n",
       "      <td>116.684453</td>\n",
       "      <td>55.580628</td>\n",
       "      <td>136.137341</td>\n",
       "      <td>70.426253</td>\n",
       "      <td>83.921694</td>\n",
       "      <td>83.736124</td>\n",
       "      <td>139.208850</td>\n",
       "      <td>145.166297</td>\n",
       "      <td>131.018160</td>\n",
       "      <td>164.619185</td>\n",
       "      <td>5.086305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>n_001</td>\n",
       "      <td>59-20151230231714-00.jpg</td>\n",
       "      <td>24.865542</td>\n",
       "      <td>28.634538</td>\n",
       "      <td>38.175412</td>\n",
       "      <td>83.921694</td>\n",
       "      <td>35.615822</td>\n",
       "      <td>99.279237</td>\n",
       "      <td>61.211727</td>\n",
       "      <td>54.742362</td>\n",
       "      <td>86.295715</td>\n",
       "      <td>137.161177</td>\n",
       "      <td>139.535198</td>\n",
       "      <td>132.553914</td>\n",
       "      <td>193.286599</td>\n",
       "      <td>5.086305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>n_001</td>\n",
       "      <td>59-20151230231720-00.jpg</td>\n",
       "      <td>23.841705</td>\n",
       "      <td>105.934174</td>\n",
       "      <td>38.175412</td>\n",
       "      <td>120.267882</td>\n",
       "      <td>42.270757</td>\n",
       "      <td>127.946653</td>\n",
       "      <td>49.949529</td>\n",
       "      <td>112.589110</td>\n",
       "      <td>73.497762</td>\n",
       "      <td>132.553916</td>\n",
       "      <td>111.379702</td>\n",
       "      <td>121.291718</td>\n",
       "      <td>126.225327</td>\n",
       "      <td>-0.544792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>n_001</td>\n",
       "      <td>59-20151230231721-00.jpg</td>\n",
       "      <td>26.913214</td>\n",
       "      <td>101.838830</td>\n",
       "      <td>37.151576</td>\n",
       "      <td>117.708291</td>\n",
       "      <td>44.318430</td>\n",
       "      <td>125.387062</td>\n",
       "      <td>51.997201</td>\n",
       "      <td>109.005683</td>\n",
       "      <td>66.842826</td>\n",
       "      <td>139.720769</td>\n",
       "      <td>104.212848</td>\n",
       "      <td>131.018162</td>\n",
       "      <td>141.070952</td>\n",
       "      <td>3.550553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   behavior image_id                image_file     head_x      head_y  \\\n",
       "0         0    n_001  59-20151230231705-00.jpg  19.234443   92.112384   \n",
       "1         0    n_001  59-20151230231706-00.jpg  12.579507   49.111263   \n",
       "2         0    n_001  59-20151230231714-00.jpg  24.865542   28.634538   \n",
       "3         0    n_001  59-20151230231720-00.jpg  23.841705  105.934174   \n",
       "4         0    n_001  59-20151230231721-00.jpg  26.913214  101.838830   \n",
       "\n",
       "   beak_base_x  beak_base_y  beak_tip_x  beak_tip_y     neck_x      neck_y  \\\n",
       "0    41.246921   134.089668   39.711167  149.447212  61.211727   86.993203   \n",
       "1    43.806512   116.684453   55.580628  136.137341  70.426253   83.921694   \n",
       "2    38.175412    83.921694   35.615822   99.279237  61.211727   54.742362   \n",
       "3    38.175412   120.267882   42.270757  127.946653  49.949529  112.589110   \n",
       "4    37.151576   117.708291   44.318430  125.387062  51.997201  109.005683   \n",
       "\n",
       "     body1_x     body1_y     body2_x     body2_y  tail_base_x  tail_base_y  \n",
       "0  79.640779  136.137341  123.153818  131.530078   176.393301     7.133978  \n",
       "1  83.736124  139.208850  145.166297  131.018160   164.619185     5.086305  \n",
       "2  86.295715  137.161177  139.535198  132.553914   193.286599     5.086305  \n",
       "3  73.497762  132.553916  111.379702  121.291718   126.225327    -0.544792  \n",
       "4  66.842826  139.720769  104.212848  131.018162   141.070952     3.550553  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_argmax(heatmaps: torch.Tensor, num_keypoints: int = 7) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Decode heatmaps into keypoint coordinates using soft-argmax.\n",
    "\n",
    "    Args:\n",
    "        heatmaps (torch.Tensor): Heatmaps of shape (batch_size, num_keypoints * 2, H, W).\n",
    "        num_keypoints (int): The number of keypoints.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Keypoint coordinates (batch_size, num_keypoints, 2).\n",
    "    \"\"\"\n",
    "    batch_size, _, H, W = heatmaps.size()\n",
    "\n",
    "    # Reshape the heatmaps to flatten the spatial dimensions (H, W)\n",
    "    heatmaps = heatmaps.view(batch_size, num_keypoints * 2, -1)\n",
    "\n",
    "    # Apply softmax along the flattened spatial dimensions\n",
    "    probabilities = F.softmax(heatmaps, dim=-1)\n",
    "\n",
    "    # Create index tensor for weighted sum\n",
    "    indices = torch.arange(H * W, device=heatmaps.device).float().view(1, 1, -1)\n",
    "\n",
    "    # Compute x and y coordinates by weighted sum\n",
    "    x_coords = (indices % W) * probabilities  # Weighted x\n",
    "    y_coords = (indices // W) * probabilities  # Weighted y\n",
    "\n",
    "    # Sum over the spatial dimensions to get the weighted average coordinates\n",
    "    x_coords = x_coords.sum(dim=-1)\n",
    "    y_coords = y_coords.sum(dim=-1)\n",
    "\n",
    "    # Return coordinates in (batch_size, num_keypoints, 2)\n",
    "    return torch.stack([x_coords, y_coords], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdPoseModel(nn.Module):\n",
    "    def __init__(self, num_keypoints: int, num_classes: int = 1, name=\"resnet50_ssdlite_topdown\"):\n",
    "        super(BirdPoseModel, self).__init__()\n",
    "        self.name = name\n",
    "\n",
    "        # Load pretrained SSDLite for bounding box detection\n",
    "        ssdlite = models.detection.ssdlite320_mobilenet_v3_large(weights='DEFAULT')\n",
    "        self.detector = ssdlite\n",
    "        self.detector.head.classification_head.num_classes = num_classes + 1  # +1: Default background class\n",
    "\n",
    "        # Keypoint heatmap generation branch (try to predict the local `num_keypoints`)\n",
    "        self.keypoint_head = nn.Sequential(\n",
    "            nn.Conv2d(1280, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),  # BatchNorm to prevent neuron co-adaptation\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_keypoints * 2, kernel_size=1)  # Output heatmaps for keypoints\n",
    "        )\n",
    "\n",
    "        # Refinement branch with upsampling [num_keypoints -> 128] for better resolution\n",
    "        self.refinement_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(num_keypoints * 2, 128, kernel_size=4, stride=2, padding=1),  # Upsampling\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, num_keypoints * 2, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, keypoints) -> dict:\n",
    "        \"\"\"\n",
    "        Forward pass for SSDLite-based bounding box detection and keypoint prediction.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing bounding boxes and keypoint coordinates.\n",
    "        \"\"\"\n",
    "        # SSDLite for bounding box detection (without ground truth)\n",
    "        detections = self.detector(x)  # List[Dict] with bounding boxes and scores\n",
    "        \n",
    "        # Extract bounding boxes and corresponding features\n",
    "        feature_maps = self.detector.backbone(x)  # Intermediate features from backbone\n",
    "        detected_bboxes = [detection[\"boxes\"] for detection in detections]  # Extract predicted bounding boxes\n",
    "\n",
    "        keypoint_coords = []\n",
    "        for i, bboxes in enumerate(detected_bboxes):\n",
    "            if len(bboxes) > 0:\n",
    "                # Crop regions using ROI Align\n",
    "                cropped_features = roi_align(feature_maps[i:i+1], [bboxes], output_size=(7, 7))\n",
    "                cropped_features = cropped_features.mean(dim=1, keepdim=True)  # Reduce channel dimension\n",
    "\n",
    "                # Generate keypoint heatmaps\n",
    "                heatmaps = self.keypoint_head(cropped_features)\n",
    "                refined_heatmaps = self.refinement_head(heatmaps)\n",
    "\n",
    "                # Decode heatmaps into keypoint coordinates\n",
    "                coords = soft_argmax(refined_heatmaps, keypoints, num_keypoints=7).squeeze() \n",
    "                keypoint_coords.append(coords)\n",
    "            else:\n",
    "                keypoint_coords.append(torch.empty(0, device=x.device))  # No detections\n",
    "\n",
    "        return {\n",
    "            \"detections\": detections,  # SSDLite bounding boxes, and scores \n",
    "            \"keypoints\": keypoint_coords  # Decoded keypoint coordinates\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Transformations Defination\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "kp_transform = pose_estimation.NormalizeKeypoints(IMG_WIDTH, IMG_HEIGHT)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = pose_estimation.PoseDataset(\n",
    "    dataframe=train_df, \n",
    "    dataset_root_folder=DATASET_ROOT, \n",
    "    img_transform=img_transform, \n",
    "    kp_transform=kp_transform\n",
    ")\n",
    "\n",
    "test_dataset = pose_estimation.PoseDataset(\n",
    "    dataframe=test_df, \n",
    "    dataset_root_folder=DATASET_ROOT, \n",
    "    img_transform=img_transform, \n",
    "    kp_transform=kp_transform\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=NUM_BATCH, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=NUM_BATCH, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BirdPoseModel(NUM_KEYPOINTS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm  # Optional: For progress bar\n",
    "\n",
    "# Loss functions for bounding boxes and keypoints\n",
    "def compute_keypoint_loss(pred_heatmaps, target_keypoints, batch_size):\n",
    "    \"\"\"\n",
    "    Computes the loss for keypoints by comparing predicted heatmaps to ground truth keypoints.\n",
    "    \n",
    "    Args:\n",
    "        pred_heatmaps (torch.Tensor): Predicted heatmaps, shape (batch_size, num_keypoints, H, W).\n",
    "        target_keypoints (torch.Tensor): Ground truth keypoints, shape (batch_size, num_keypoints, 2).\n",
    "        batch_size (int): Batch size.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Computed keypoint loss.\n",
    "    \"\"\"\n",
    "    # Using mean squared error (MSE) for keypoints\n",
    "    keypoint_loss = 0\n",
    "    for i in range(batch_size):\n",
    "        for j in range(target_keypoints.shape[1]):\n",
    "            # For each keypoint, calculate MSE between predicted heatmap and ground truth\n",
    "            pred_coords = soft_argmax(pred_heatmaps[i, j:j+1])  # Predicted keypoint from soft-argmax\n",
    "            true_coords = target_keypoints[i, j]\n",
    "            keypoint_loss += F.mse_loss(pred_coords, true_coords)\n",
    "    \n",
    "    return keypoint_loss / (batch_size * target_keypoints.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/73 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "targets should not be none when in training mode",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m keypoints\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeypoints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m detections \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetections\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     18\u001b[0m predicted_keypoints \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\quods\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\quods\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[9], line 41\u001b[0m, in \u001b[0;36mBirdPoseModel.forward\u001b[1;34m(self, x, keypoints)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03mForward pass for SSDLite-based bounding box detection and keypoint prediction.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    dict: Dictionary containing bounding boxes and keypoint coordinates.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# SSDLite for bounding box detection (without ground truth)\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# List[Dict] with bounding boxes and scores\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Extract bounding boxes and corresponding features\u001b[39;00m\n\u001b[0;32m     44\u001b[0m feature_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetector\u001b[38;5;241m.\u001b[39mbackbone(x)  \u001b[38;5;66;03m# Intermediate features from backbone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\quods\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\quods\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\quods\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torchvision\\models\\detection\\ssd.py:331\u001b[0m, in \u001b[0;36mSSD.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtargets should not be none when in training mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets:\n",
      "File \u001b[1;32mc:\\Users\\quods\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\__init__.py:2041\u001b[0m, in \u001b[0;36m_assert\u001b[1;34m(condition, message)\u001b[0m\n\u001b[0;32m   2035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(condition) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;129;01mand\u001b[39;00m overrides\u001b[38;5;241m.\u001b[39mhas_torch_function(\n\u001b[0;32m   2036\u001b[0m     (condition,)\n\u001b[0;32m   2037\u001b[0m ):\n\u001b[0;32m   2038\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m overrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[0;32m   2039\u001b[0m         _assert, (condition,), condition, message\n\u001b[0;32m   2040\u001b[0m     )\n\u001b[1;32m-> 2041\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n",
      "\u001b[1;31mAssertionError\u001b[0m: targets should not be none when in training mode"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10  # Number of epochs for training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, keypoints in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        images = images.to(device)\n",
    "        keypoints = keypoints.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, keypoints)\n",
    "\n",
    "        detections = outputs[\"detections\"]\n",
    "        predicted_keypoints = outputs[\"keypoints\"]\n",
    "\n",
    "        # Flatten keypoints\n",
    "        keypoints = keypoints.view(-1, NUM_KEYPOINTS, 2)\n",
    "        predicted_keypoints = torch.stack(predicted_keypoints).view(-1, NUM_KEYPOINTS, 2)\n",
    "\n",
    "        # Compute loss (L2 loss for keypoint predictions)\n",
    "        loss = F.mse_loss(predicted_keypoints, keypoints)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
